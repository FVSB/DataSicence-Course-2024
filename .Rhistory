(nrow(train_table)-nrow(without_outliders_trainTrable))
hist(train_table$Avgexp, col="purple", border = F,
main = "Histograma de Avgexp del cliente",
xlab = "Valor", ylab = "Frecuencia")
hist(without_outliders_trainTrable$Avgexp, col="purple", border = F,
main = "Histograma de Avgexp del cliente",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Inc_per, col="purple", border = F,
main = "Histograma de Inc_per del cliente",
xlab = "Valor", ylab = "Frecuencia")
hist(without_outliders_trainTrable$Inc_per, col="purple", border = F,
main = "Histograma de Inc_per del cliente",
xlab = "Valor", ylab = "Frecuencia")
model_logistReg_Interc <- glm(default ~ 1, data=without_outliders_trainTrable, family=binomial(link="logit") )
# Modelo m?s complejo a generar: con todas las variables
# No vamos a tener en cuenta Exp_Inc x su correlacion con Avgexp
model_logist_All <- glm(default ~ Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major, data=without_outliders_trainTrable, family=binomial(link="logit") )
modelo.regresionLogistica.stepwise <- step(model_logistReg_Interc, scope=list(lower=model_logistReg_Interc,
upper=model_logist_All), direction="both")
modelo.regresionLogistica.stepwise
# install.packages("lmtest")
library(lmtest)
coeftest(modelo.regresionLogistica.stepwise)
predictTest.logisticaStepwise <- predict(modelo.regresionLogistica.stepwise,
newdata=test_table,
type="response")
install.packages("gplots")
library(gplots)
install.packages("ROCR")
library(ROCR)
predict_TestAux_logisticStepwise <- prediction(predictTest.logisticaStepwise,
test_table$default,
label.ordering = NULL)
roc_CurveTest_logisticStepwise <- performance(predict_TestAux_logisticStepwise, "tpr", "fpr")
plot(roc_CurveTest_logisticStepwise, main="Curva ROC", colorize=TRUE)
# L?nea base
abline(a=0,b=1,col="black")
auc_ROC<-performance(predict_TestAux_logisticStepwise, "auc")
auc_ROC@y.values[[1]]
precision<-performance(predict_TestAux_logisticStepwise, "prec", "rpp")
plot(precision, main="Gr?fico de precisi?n", colorize=T)
prior=sum(train_table$default==1)/length(train_table$default)
print(prior)
# L?nea base
abline(a=prior,b=0,col="black")
coverage<-performance(predict_TestAux_logisticStepwise, "rec", "rpp")
install.packages("ROCR")
install.packages("ROCR")
precision<-performance(predict_TestAux_logisticStepwise, "prec", "rpp")
plot(precision, main="Gr?fico de precisi?n", colorize=T)
prior=sum(train_table$default==1)/length(train_table$default)
print(prior)
# L?nea base
abline(a=prior,b=0,col="black")
coverage<-performance(predict_TestAux_logisticStepwise, "rec", "rpp")
plot(coverage, main="Gráfico de cobertura", colorize=T)
# L?nea base
abline(a=0,b=1,col="black")
lift<-performance(predict_TestAux_logisticStepwise, "lift", "rpp")
plot(lift, main="Gr?fico lift", colorize=T)
# L?nea base
abline(a=1,b=0,col="black")
install.packages("partykit")
library(partykit)
`dependencies tree.fit` <- ctree(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, control=ctree_control(alpha=0.01, maxdepth=3, minbucket = 30))
graphics.off()
plot(`dependencies tree.fit`)
install.packages("rpart")
library(rpart)
Gini_tree.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(split = "gini"), control= rpart.control(minbucket = 30, maxdepth = 3, cp=0.05))
Gini_tree.fit
Gini_tree.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(split = "gini"), control= rpart.control(minbucket = 30, maxdepth = 3, cp=0.002))
Gini_tree.fit
Gini_tree.fit.without_prune <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(split = "gini"), control= rpart.control(minbucket = 30, maxdepth = 3, cp=-1))
Gini_tree.fit.without_prune
install.packages("rattle")
##################################################
##################################################
# 5. Valoraci?n: ROC, Cobertura, Precisi?n, LIFT #
##################################################
##################################################
test_table$Depndt <- as.factor(test_table$Depndt)
print(test_table$Depndt)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
print(RocTest.tree)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
print(RocTest.tree)
plot(RocTest.tree, main="Curva ROC", colorize=TRUE)
# L?nea base
abline(a=0,b=1,col="black")
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
auc_ROC@y.values[[1]]
auc_ROC@y.values[[1]]
precision<-performance(predict_TestAuxTable.tree, "prec", "rpp")
plot(precision, main="Gr?fico de precisi?n", colorize=T)
prior=sum(train_table$default==1)/length(train_table$default)
print(prior)
# L?nea base
abline(a=prior,b=0,col="black")
coverage<-performance(predict_TestAuxTable.tree, "rec", "rpp")
plot(coverage, main="Gr?fico de cobertura", colorize=T)
# L?nea base
abline(a=0,b=1,col="black")
lift<-performance(predict_TestAuxTable.tree, "lift", "rpp")
plot(lift, main="Gr?fico lift", colorize=T)
# L?nea base
abline(a=1,b=0,col="black")
loss_matrix <- matrix(c(0, 360, 10, 0), byrow = TRUE, nrow = 2)
arbolGini_Cost.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(prior = c(.025,.975),split = "Gini", loss=loss_matrix),
control= rpart.control(minbucket = 30, maxdepth = 3, cp=-1))
graphics.off()
fancyRpartPlot(arbolGini_Cost.fit,caption=NULL,palettes=c("Purples"))
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
plot(RocTest.tree, main="Curva ROC", colorize=TRUE)
# L?nea base
abline(a=0,b=1,col="black")
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
auc_ROC@y.values[[1]]
precision<-performance(predict_TestAuxTable.tree, "prec", "rpp")
plot(precision, main="Gr?fico de precisi?n", colorize=T)
prior=sum(train_table$default==1)/length(train_table$default)
print(prior)
# L?nea base
abline(a=prior,b=0,col="black")
coverage<-performance(predict_TestAuxTable.tree, "rec", "rpp")
plot(coverage, main="Gr?fico de cobertura", colorize=T)
# L?nea base
abline(a=0,b=1,col="black")
lift<-performance(predict_TestAuxTable.tree, "lift", "rpp")
plot(lift, main="Gr?fico lift", colorize=T)
# L?nea base
abline(a=1,b=0,col="black")
install.packages('randomForest')
library(randomForest)
# Nota: El Random Forest, no funciona con missings.
set.seed(12345)
model.randomForest <- randomForest(formula=default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table,
ntree=100,
nodesize=30, # observaciones por hoja
maxnodes=8, # 2^3 = 8 (profundidad = 3)
# Para valorar la importancia de cada uno de los 500 ?rboles #
importance=TRUE,
keep.forest=TRUE)
graphics.off()
varImpPlot(model.randomForest)
predict_TestTable.randomForest <- predict(model.randomForest, type='prob', test_table)
graphics.off()
varImpPlot(model.randomForest)
predict_TestTable.randomForest <- predict(model.randomForest, type='prob', test_table)
predict_AuxTable.randomForest <- prediction(predict_TestTable.randomForest[, 2], test_table$default)
auc_ROC<-performance(predict_AuxTable.randomForest, "auc")
install.packages('randomForest')
install.packages("randomForest")
rm(list=ls())
# Obtiene el directorio de trabajo actual
directorio_actual <- getwd()
setwd(directorio_actual)
train_table <- read.csv("datamart_morosidad_training.csv", fileEncoding="utf-8", stringsAsFactors = TRUE)
print(train_table)
test_table <- read.csv("datamart_morosidad_test.csv", fileEncoding="utf-8", stringsAsFactors = TRUE)
print(test_table)
summary(train_table)
install.packages("moments")
library(moments)
descriptivoManual <- function(explanatory_Table){
# Obtiene los nombres de las variables de la tabla de datos
vars_names <- names(explanatory_Table)
# Itera sobre cada nombre de variable
for (i in vars_names){
# Imprime el nombre de la variable
print(paste0("Variable: ",i))
# Imprime el tipo de variable (factor, numérico, etc.)
print(paste0("Tipo de variable: ", class(explanatory_Table[, i])))
# Verifica si la variable es de tipo factor
if (class(explanatory_Table[, i])=="factor"){
# Calcula la moda de la variable factor
valores <- unique(explanatory_Table[, i])
moda <- valores[which.max(tabulate(match(explanatory_Table[,i], valores)))]
print(paste0("Moda: ", moda))
# Imprime el número de niveles de la variable factor
print(paste0("Nº de niveles: ", length(valores)))
# Calcula y muestra el porcentaje de valores faltantes (NA)
print(paste0("% de missings: ", 100*(sum(is.na(explanatory_Table[, i]))/length(explanatory_Table[, i])), "%"))
} else{
# Si la variable no es factor, calcula y muestra estadísticas descriptivas numéricas
print(paste0("Mínimo: ", min(explanatory_Table[, i], na.rm = T)))
print(paste0("Máximo: ", max(explanatory_Table[, i], na.rm = T)))
print(paste0("Media: ", mean(explanatory_Table[, i], na.rm = T)))
print(paste0("Mediana: ", median(explanatory_Table[, i], na.rm = T)))
print(paste0("Desviación típica: ", sd(explanatory_Table[, i], na.rm = T)))
print(paste0("Asimetría: ", skewness(explanatory_Table[, i], na.rm = T)))
print(paste0("Curtosis: ", kurtosis(explanatory_Table[, i], na.rm = T)))
# Calcula y muestra el porcentaje de valores faltantes (NA)
print(paste0("% de missings: ", 100*(sum(is.na(explanatory_Table[, i]))/length(explanatory_Table[, i])), "%"))
}
# Imprime una línea divisoria para separar la salida de cada variable
print("*********************************************")
}
}
descriptivoManual(train_table[, c("Age", "Avgexp", "Income", "Inc_per", "Cur_add", "Active")])
class(train_table)
hist(train_table$Avgexp, col="purple", border = F,
main = "Histograma de Avgexp del cliente",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Age, col="purple", border = F,
main = "Age",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Income, col="purple", border = F,
main = "Income",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Inc_per, col="purple", border = F,
main = "Inc_per",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Cur_add, col="purple", border = F,
main = "Curr_add",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Active, col="purple", border = F,
main = "Active",
xlab = "Valor", ylab = "Frecuencia")
boxplot(train_table$Avgexp, col = "purple", main = "Boxplot de Avgexp")
boxplot(train_table$Inc_per, col = "purple", main = "Boxplot de Inc_per")
correlationMatrix<-cor(train_table[, c("Avgexp", "Age", "Income", "Inc_per")], method = c("pearson"))
graphics.off()
install.packages("corrplot")
library(corrplot)
install.packages("dplyr")
library(dplyr)
install.packages("ggcorrplot")
library(ggcorrplot)
correlationMatrix<-cor(train_table[c("Avgexp", "Age", "Income", "Inc_per", "Active", "Cur_add", "Ownrent", "Selfempl", "Exp_Inc", "Depndt", "Major")], method = c("pearson"))
corrplot(correlationMatrix, method="number", type="upper", tl.cex=0.5)
train_table$default <- as.factor(train_table$default)
train_table$Depndt <- as.factor(train_table$Depndt)
train_table$Depndt<-as.factor(train_table$Depndt)
train_table$default<-as.factor(train_table$default)
tablaFrecuencia<-table(train_table$default, train_table$Active)
barplot(tablaFrecuencia, col = c('springgreen1','purple'), border = F)
tablaFrecuencia<-table(train_table$default, train_table$Depndt)
barplot(tablaFrecuencia, col = c('springgreen1','purple'), border = F)
table(train_table$default)
set.seed(12345)
undersampling <- function(data, objective_var, ones_proportion_wish){
`1s_counts` <- nrow(subset(data, objective_var==1))
`0s_counts` <- round((`1s_counts`/ones_proportion_wish)-`1s_counts`)
`0sstable` <- subset(data, objective_var==0)
zeros <- sample(nrow(`0sstable`), `0s_counts`, replace = FALSE)
return(rbind(data[variableObjetivo==1,], `0sstable`[ceros,]))
}
undersampling_0.5 <- undersampling(train_table, train_table$default, 1/2)
graphics.off()
install.packages('randomForest')
library(randomForest)
# Nota: El Random Forest, no funciona con missings.
set.seed(12345)
model.randomForest <- randomForest(formula=default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table,
ntree=100,
nodesize=30, # observaciones por hoja
maxnodes=8, # 2^3 = 8 (profundidad = 3)
# Para valorar la importancia de cada uno de los 500 ?rboles #
importance=TRUE,
keep.forest=TRUE)
graphics.off()
varImpPlot(model.randomForest)
predict_TestTable.randomForest <- predict(model.randomForest, type='prob', test_table)
predict_AuxTable.randomForest <- prediction(predict_TestTable.randomForest[, 2], test_table$default)
auc_ROC<-performance(predict_AuxTable.randomForest, "auc")
auc_ROC@y.values[[1]]
rm(list=ls())
# Obtiene el directorio de trabajo actual
directorio_actual <- getwd()
setwd(directorio_actual)
train_table <- read.csv("datamart_morosidad_training.csv", fileEncoding="utf-8", stringsAsFactors = TRUE)
print(train_table)
test_table <- read.csv("datamart_morosidad_test.csv", fileEncoding="utf-8", stringsAsFactors = TRUE)
print(test_table)
summary(train_table)
install.packages("moments")
library(moments)
descriptivoManual(train_table[, c("Age", "Avgexp", "Income", "Inc_per", "Cur_add", "Active")])
class(train_table)
hist(train_table$Avgexp, col="purple", border = F,
main = "Histograma de Avgexp del cliente",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Age, col="purple", border = F,
main = "Age",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Income, col="purple", border = F,
main = "Income",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Inc_per, col="purple", border = F,
main = "Inc_per",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Cur_add, col="purple", border = F,
main = "Curr_add",
xlab = "Valor", ylab = "Frecuencia")
hist(train_table$Active, col="purple", border = F,
main = "Active",
xlab = "Valor", ylab = "Frecuencia")
boxplot(train_table$Avgexp, col = "purple", main = "Boxplot de Avgexp")
boxplot(train_table$Inc_per, col = "purple", main = "Boxplot de Inc_per")
correlationMatrix<-cor(train_table[, c("Avgexp", "Age", "Income", "Inc_per")], method = c("pearson"))
graphics.off()
install.packages("corrplot")
library(corrplot)
install.packages("dplyr")
library(dplyr)
install.packages("ggcorrplot")
library(ggcorrplot)
correlationMatrix<-cor(train_table[c("Avgexp", "Age", "Income", "Inc_per", "Active", "Cur_add", "Ownrent", "Selfempl", "Exp_Inc", "Depndt", "Major")], method = c("pearson"))
corrplot(correlationMatrix, method="number", type="upper", tl.cex=0.5)
train_table$default <- as.factor(train_table$default)
train_table$Depndt <- as.factor(train_table$Depndt)
train_table$Depndt<-as.factor(train_table$Depndt)
train_table$default<-as.factor(train_table$default)
tablaFrecuencia<-table(train_table$default, train_table$Active)
barplot(tablaFrecuencia, col = c('springgreen1','purple'), border = F)
tablaFrecuencia<-table(train_table$default, train_table$Depndt)
barplot(tablaFrecuencia, col = c('springgreen1','purple'), border = F)
table(train_table$default)
set.seed(12345)
undersampling <- function(data, objective_var, ones_proportion_wish){
`1s_counts` <- nrow(subset(data, objective_var==1))
`0s_counts` <- round((`1s_counts`/ones_proportion_wish)-`1s_counts`)
`0sstable` <- subset(data, objective_var==0)
zeros <- sample(nrow(`0sstable`), `0s_counts`, replace = FALSE)
return(rbind(data[variableObjetivo==1,], `0sstable`[ceros,]))
}
undersampling_0.5 <- undersampling(train_table, train_table$default, 1/2)
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
lift<-performance(predict_TestAuxTable.tree, "lift", "rpp")
plot(lift, main="Gr?fico lift", colorize=T)
# L?nea base
abline(a=1,b=0,col="black")
loss_matrix <- matrix(c(0, 360, 10, 0), byrow = TRUE, nrow = 2)
arbolGini_Cost.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(prior = c(.025,.975),split = "Gini", loss=loss_matrix),
control= rpart.control(minbucket = 30, maxdepth = 3, cp=-1))
graphics.off()
fancyRpartPlot(arbolGini_Cost.fit,caption=NULL,palettes=c("Purples"))
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
plot(RocTest.tree, main="Curva ROC", colorize=TRUE)
# L?nea base
abline(a=0,b=1,col="black")
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
graphics.off()
fancyRpartPlot(arbolGini_Cost.fit,caption=NULL,palettes=c("Purples"))
loss_matrix <- matrix(c(0, 360, 10, 0), byrow = TRUE, nrow = 2)
arbolGini_Cost.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(prior = c(.025,.975),split = "Gini", loss=loss_matrix),
control= rpart.control(minbucket = 30, maxdepth = 3, cp=-1))
# Inclusi?n de costes por error (matriz de costee beneficios)
# Loss matrix: 0,1 por columnas y -,+ por filas
# Si digo "-" y era 1 (pierdo una baja), mayor coste
# Si digo "+" y era 0 (hago una campa?a inncesaria), menor coste
# NOTA: Loss matrix must have zero on diagonals
# Cargar el paquete rpart
library(rpart)
loss_matrix <- matrix(c(0, 360, 10, 0), byrow = TRUE, nrow = 2)
arbolGini_Cost.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(prior = c(.025,.975),split = "Gini", loss=loss_matrix),
control= rpart.control(minbucket = 30, maxdepth = 3, cp=-1))
graphics.off()
fancyRpartPlot(arbolGini_Cost.fit,caption=NULL,palettes=c("Purples"))
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
plot(RocTest.tree, main="Curva ROC", colorize=TRUE)
# L?nea base
abline(a=0,b=1,col="black")
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
arbolGini_Cost.fit <- rpart(default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table, method="class",
parms = list(prior = c(.025,.975),split = "Gini", loss=loss_matrix),
control= rpart.control(minbucket = 30, maxdepth = 3, cp=-1))
graphics.off()
fancyRpartPlot(arbolGini_Cost.fit,caption=NULL,palettes=c("Purples"))
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
graphics.off()
fancyRpartPlot(arbolGini_Cost.fit,caption=NULL,palettes=c("Purples"))
# Asegurarse de que 'Depndt' sea un factor en test_table
test_table$Depndt <- as.factor(test_table$Depndt)
predict_TestTable.tree <-predict(arbolGini_Cost.fit, type='prob', test_table)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
# Instalar el paquete ROCR si aún no lo has hecho
install.packages("ROCR")
# Cargar el paquete ROCR
library(ROCR)
predict_TestAuxTable.tree <- prediction(predict_TestTable.tree[, 2], test_table$default)
RocTest.tree <- performance(predict_TestAuxTable.tree, "tpr", "fpr")
plot(RocTest.tree, main="Curva ROC", colorize=TRUE)
# L?nea base
abline(a=0,b=1,col="black")
auc_ROC<-performance(predict_TestAuxTable.tree, "auc")
auc_ROC@y.values[[1]]
auc_ROC@y.values[[1]]
precision<-performance(predict_TestAuxTable.tree, "prec", "rpp")
plot(precision, main="Gr?fico de precisi?n", colorize=T)
prior=sum(train_table$default==1)/length(train_table$default)
print(prior)
# L?nea base
abline(a=prior,b=0,col="black")
coverage<-performance(predict_TestAuxTable.tree, "rec", "rpp")
plot(coverage, main="Gr?fico de cobertura", colorize=T)
# L?nea base
abline(a=0,b=1,col="black")
lift<-performance(predict_TestAuxTable.tree, "lift", "rpp")
plot(lift, main="Gr?fico lift", colorize=T)
# L?nea base
abline(a=1,b=0,col="black")
install.packages('randomForest')
library(randomForest)
# Nota: El Random Forest, no funciona con missings.
set.seed(12345)
model.randomForest <- randomForest(formula=default~Avgexp+Age+Income+Inc_per+Active+Cur_add+Ownrent+Selfempl+Depndt+Major,
data=train_table,
ntree=100,
nodesize=30, # observaciones por hoja
maxnodes=8, # 2^3 = 8 (profundidad = 3)
# Para valorar la importancia de cada uno de los 500 ?rboles #
importance=TRUE,
keep.forest=TRUE)
graphics.off()
varImpPlot(model.randomForest)
predict_TestTable.randomForest <- predict(model.randomForest, type='prob', test_table)
predict_AuxTable.randomForest <- prediction(predict_TestTable.randomForest[, 2], test_table$default)
auc_ROC<-performance(predict_AuxTable.randomForest, "auc")
auc_ROC@y.values[[1]]
auc_ROC@y.values[[1]]
# En este caso, vamos a contemplar una tabla de validaci?n
install.packages("lubridate")
install.packages("caret")
library(caret)
library(caret)
library(lattice)
trainProportion <- 0.7
trainIndexes <- createDataPartition(1:nrow(train_table), 1, trainProportion, list = FALSE)
training <- train_table[trainIndexes,]
validation <- train_table[-trainIndexes,]
install.packages('data.table')
install.packages("data.table")
install.packages('data.table')
library(data.table)
training.dt<-data.table(training)
install.packages('mltools')
library(mltools)
one_hot_encoding.training<-one_hot(training.dt[,c("Avgexp","Age","Income","Inc_per","Active","Cur_add","Ownrent","Selfempl","Exp_Inc","Depndt","Major")],dropCols=TRUE)
validation.dt<-data.table(validation)
one_hot_encoding.validation<-one_hot(validation.dt[,c("Avgexp","Age","Income","Inc_per","Active","Cur_add","Ownrent","Selfempl","Exp_Inc","Depndt","Major")],dropCols=TRUE)
test.dt<-data.table(test_table)
one_hot_encoding.test<-one_hot(test.dt[,c("Avgexp","Age","Income","Inc_per","Active","Cur_add","Ownrent","Selfempl","Exp_Inc","Depndt","Major")],dropCols=TRUE)
install.packages('xgboost')
library(xgboost)
one_hot_encoding.training.DMatrix<-xgb.DMatrix(
data.matrix(one_hot_encoding.training),
label=training$default
)
one_hot_encoding.validation.DMatrix<-xgb.DMatrix(
data.matrix(one_hot_encoding.validation),
label=validation$default
)
one_hot_encoding.test.DMatrix<-xgb.DMatrix(
data.matrix(one_hot_encoding.test),
label=test_table$default
)
set.seed(12345)
watchlist <- list(train=one_hot_encoding.training.DMatrix,
valid=one_hot_encoding.validation.DMatrix)
hyperparameters<-list(subsample = 0.75, # Porcentaje de observaciones de cada muestra
max_depth = 10, # Profundidad de los ?rboles
eta = 0.05) # Tasa de aprendizaje)
xgb.model <- xgb.train(one_hot_encoding.training.DMatrix,
params=hyperparameters,
nrounds = 100,
watchlist=watchlist,
early_stopping_rounds = 10)
predictTest.xgb <- as.data.frame(predict(xgb.model,one_hot_encoding.test.DMatrix))
predictTestAux.xgb <- prediction(predictTest.xgb[,1], test_table$default)
auc_ROC<-performance(predictTestAux.xgb, "auc")
auc_ROC@y.values[[1]]
# 0.796999 > 0.6481545 (Regresion continua arrojando mejores resultados)
# 0.796999 > 0.6481545 (Regresion continua arrojando mejores resultados)
# Aun probando con varios valores con cada uno de los hiperparametros uno de los
# 0.796999 > 0.6481545 (Regresion continua arrojando mejores resultados)
# Aun probando con varios valores con cada uno de los hiperparametros uno de los
# mejores resultados fue de  0.6481545
